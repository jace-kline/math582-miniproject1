{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c77a5b",
   "metadata": {},
   "source": [
    "# Mini-project 1: Principal Component Analysis (PCA)\n",
    "\n",
    "## Algorithm Development Steps\n",
    "\n",
    "In this section, we walk through the steps behind implementing the PCA algorithm outlined in section 10.6 of the textbook. The algorithm will consist of four main steps:\n",
    "\n",
    "1. Centering the original data set samples around the origin\n",
    "2. Standardizing the data set based on standard deviation\n",
    "3. Computing the eigendecomposition of the covariance matrix from the centered, standardized data set\n",
    "4. Choosing the dimension for approximating the original space\n",
    "\n",
    "\n",
    "#### Assumptions\n",
    "\n",
    "* The data matrix X is structured such that rows are attributes and columns are samples\n",
    "* The number of rows in data matrix X is less than the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ce0229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.decomposition as skl\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src/')\n",
    "from PCA import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e1ca6",
   "metadata": {},
   "source": [
    "We will use the following data matrix below throughout the development of the algorithm to demonstrate intermediate steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6c77803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 8 2 1 5]\n",
      " [8 7 2 2 6]\n",
      " [4 0 5 0 4]]\n",
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "# sample data matrix for testing/demonstration\n",
    "\n",
    "A = np.array([\n",
    "    [2,8,2,1,5],\n",
    "    [8,7,2,2,6],\n",
    "    [4,0,5,0,4]\n",
    "])\n",
    "\n",
    "print(A)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a0e3c",
   "metadata": {},
   "source": [
    "### Step 1: Centering the Data Matrix\n",
    "\n",
    "Per the algorithm for PCA in section 10.6 of the book, we must first center the data matrix so that each dimension has a mean of 0. We achieve this by computing the mean of each dimension (row) and then subtracting all elements in each row by the respective row's mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57648ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.6  4.4 -1.6 -2.6  1.4]\n",
      " [ 3.   2.  -3.  -3.   1. ]\n",
      " [ 1.4 -2.6  2.4 -2.6  1.4]]\n"
     ]
    }
   ],
   "source": [
    "D, samples = A.shape\n",
    "rowmeans = np.mean(A, axis=1)\n",
    "offsetmatrix = np.repeat(rowmeans, samples, axis=0).reshape((D,samples))\n",
    "centered = A - offsetmatrix\n",
    "\n",
    "print(centered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ce1581",
   "metadata": {},
   "source": [
    "### Step 2: Standardization\n",
    "\n",
    "The next step in the PCA algorithm involves standardizing each component of the data matrix by dividing by the component's respective standard deviation. We show this behavior below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7108a5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.62092042  1.70753116 -0.62092042 -1.00899568  0.54330537]\n",
      " [ 1.18585412  0.79056942 -1.18585412 -1.18585412  0.39528471]\n",
      " [ 0.64993368 -1.2070197   1.11417203 -1.2070197   0.64993368]]\n"
     ]
    }
   ],
   "source": [
    "rowstds = np.std(centered, axis=1)\n",
    "standardized = (centered.T / rowstds).T\n",
    "\n",
    "print(standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bb5f35",
   "metadata": {},
   "source": [
    "### Step 3: Eigendecomposition of the Covariance Matrix\n",
    "\n",
    "We must first find the covariance matrix of the centered and standardized data array. We then compute the eigendecomposition of this covariance matrix. Following this, we can choose a desired number of dimensions to use for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98ed0101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.25        0.69030098 -0.39635072]\n",
      " [ 0.69030098  1.25        0.04587658]\n",
      " [-0.39635072  0.04587658  1.25      ]]\n"
     ]
    }
   ],
   "source": [
    "covmatrix = np.cov(standardized)\n",
    "print(covmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0b41939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.026786  1.2895867 0.4336273]\n",
      "[[ 0.71551818 -0.02918716 -0.69798413]\n",
      " [ 0.61644272  0.4964592   0.61116826]\n",
      " [-0.32868237  0.86756923 -0.3732178 ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the eigenvalues and unit-length eigenvectors of the standardized covariance matrix\n",
    "# The eigenvalues and vectors are ordered in ascending order\n",
    "\n",
    "res = np.linalg.eigh(covmatrix)\n",
    "\n",
    "# flip the order so the eigenvalues and vectors are sorted in descending order based on eigenvalue\n",
    "eigvals = np.flip(res[0])\n",
    "eigvecs = np.flip(res[1], axis=1)\n",
    "\n",
    "print(eigvals)\n",
    "print(eigvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e799ed",
   "metadata": {},
   "source": [
    "We show below how we execute dimensionality reduction. We simply choose the same number of eigenvectors (descending order by weight) as the number dimensions we desire, and we represent these in a matrix as column vectors. We call this matrix B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c6177ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.71551818 -0.02918716]\n",
      " [ 0.61644272  0.4964592 ]\n",
      " [-0.32868237  0.86756923]]\n"
     ]
    }
   ],
   "source": [
    "DESIRED_DIMENSIONS = 2\n",
    "\n",
    "B = eigvecs[:, 0:DESIRED_DIMENSIONS]\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d665dd01",
   "metadata": {},
   "source": [
    "### Step 4: Projection\n",
    "\n",
    "Using our dimension-reducing matrix, we can take vectors from the original space and project them onto the principal subspace with fewer dimensions than the original space. To express this projection in the original space, we multiply by the original standard deviation and add the mean of each for each vector component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c85117b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [1 1 1]\n",
      "x_standardized = [-1.00899568 -1.58113883 -0.74278135]\n",
      "x_principal = [-0.99842797 -1.59039212 -0.73713071]\n",
      "x_approx = [1.02723108 0.97659083 1.01217185]\n"
     ]
    }
   ],
   "source": [
    "# x = a vector from the original space\n",
    "\n",
    "x = np.array([1,1,1])\n",
    "\n",
    "\n",
    "# x_standardized = x tranformed into centered, standardized space\n",
    "\n",
    "x_standardized = (x - rowmeans) / rowstds\n",
    "\n",
    "\n",
    "# x_principal = the vector obtained by transforming x_standardized into the principal subspace\n",
    "\n",
    "x_principal = B @ (B.T @ x_standardized)\n",
    "\n",
    "\n",
    "# x_approx = x_principal transformed back into the original space ~> An approximation of x after dimension reduction\n",
    "\n",
    "x_approx = (x_principal * rowstds) + rowmeans\n",
    "\n",
    "print(\"x =\", x)\n",
    "print(\"x_standardized =\", x_standardized)\n",
    "print(\"x_principal =\", x_principal)\n",
    "print(\"x_approx =\", x_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84d260e",
   "metadata": {},
   "source": [
    "We see that in this example the approximation is quite close to the original sample after dimension reduction. \n",
    "\n",
    "## Algorithm Implementation\n",
    "\n",
    "Now that we have illustrated the steps for the algorithm, we show the implementation of this procedure as a Python class.\n",
    "\n",
    "```python\n",
    "# Python class to express the behavior of PCA analysis\n",
    "class PCA:\n",
    "    # initialize the PCA class with a given data set X (required)\n",
    "    # optionally supply N, the number of reduction dimensions\n",
    "    def __init__(self, X, N=1):\n",
    "        # set the class variables\n",
    "        self.X = X\n",
    "        self.N = N\n",
    "\n",
    "        # center the data matrix\n",
    "        D, samples = X.shape\n",
    "        self.rowmeans = np.mean(X, axis=1)\n",
    "        self.centered = X - np.repeat(self.rowmeans, samples, axis=0).reshape((D,samples))\n",
    "\n",
    "        # standardize the centered data matrix\n",
    "        self.rowstds = np.std(self.centered, axis=1)\n",
    "        self.standardized = (self.centered.T / self.rowstds).T\n",
    "\n",
    "        # compute the covariance matrix\n",
    "        self.covmatrix = np.cov(self.standardized)\n",
    "\n",
    "        # compute the eigendecomposition of the covariance matrix\n",
    "        res = np.linalg.eigh(self.covmatrix)\n",
    "        self.eigvals = np.flip(res[0])\n",
    "        self.eigvecs = np.flip(res[1], axis=1)\n",
    "\n",
    "        # compute B\n",
    "        self.B = self.eigvecs[:, 0:self.N]\n",
    "\n",
    "    # set N, the number of dimensions to reduce to\n",
    "    def set_N(self, N):\n",
    "        # set N, recompute B\n",
    "        self.N = N\n",
    "        self.B = self.eigvecs[:, 0:self.N]\n",
    "\n",
    "    # center and standardize variance to 1\n",
    "    def standardize_sample(self, x):\n",
    "        return (x - self.rowmeans) / self.rowstds\n",
    "\n",
    "    # shift sample back to original data space\n",
    "    def unstandardize_sample(self, x):\n",
    "        return (x * self.rowstds) + self.rowmeans\n",
    "\n",
    "    # return the covariance matrix of the centered, standardized data\n",
    "    def get_covariance_matrix(self):\n",
    "        return self.covmatrix\n",
    "\n",
    "    # transform a standardized sample of D dimensions into N dimensions\n",
    "    def transform_reduce(self, x):\n",
    "        return self.B.T @ x\n",
    "\n",
    "    # transform a dimension-reduced sample of N dimensions into D dimensions\n",
    "    # the result is centered and standardized\n",
    "    def transform_inverse(self, z):\n",
    "        return self.B @ z\n",
    "\n",
    "    # perform end-to-end transformation\n",
    "    # centers, standardizes, reduces, inverts, and unstandardizes\n",
    "    # this function takes a sample and \"approximates\" it using PCA with given N\n",
    "    def transform(self, x):\n",
    "        x_standardized = self.standardize_sample(x)\n",
    "        x_principal = self.transform_inverse(self.transform_reduce(x_standardized))\n",
    "        x_transformed = self.unstandardize_sample(x_principal)\n",
    "        return x_transformed\n",
    "```\n",
    "\n",
    "### Implementation Testing\n",
    "\n",
    "Now that we have an implementation, we can test this implementation against the SciKit Learn implementation of PCA to ensure we get the same result on a test sample. The difference between our implementation and the SciKit Learn implementation is that our implementation automatically centers and standardizes the dataset prior to calculating the covariance matrix and eigendecompositions. Hence, we must have SciKit Learn's implementation perform PCA on the standardized data set, and we also must perform standardization on the sample point and undo this standardization after performing transformation. Below, we show the results of dimension reduction against sample x=(1,1,1) defined above. We see that both implementations compute the same covariance matrix and transformed sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adbada4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.25        0.69030098 -0.39635072]\n",
      " [ 0.69030098  1.25        0.04587658]\n",
      " [-0.39635072  0.04587658  1.25      ]]\n",
      "[1.02723108 0.97659083 1.01217185]\n"
     ]
    }
   ],
   "source": [
    "# Our implementation of PCA dimension reduction\n",
    "\n",
    "pca = PCA(A, N=DESIRED_DIMENSIONS)\n",
    "print(pca.get_covariance_matrix())\n",
    "print(pca.transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8548451f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.25        0.69030098 -0.39635072]\n",
      " [ 0.69030098  1.25        0.04587658]\n",
      " [-0.39635072  0.04587658  1.25      ]]\n",
      "[[1.02723108 0.97659083 1.01217185]]\n"
     ]
    }
   ],
   "source": [
    "# SciKit Learn implementation of PCA dimension reduction\n",
    "# This implementation does not automatically center and standardize the data set\n",
    "# Hence, we must perform the PCA on the centered, standardized dataset\n",
    "\n",
    "pca_skl = skl.PCA(n_components=DESIRED_DIMENSIONS, svd_solver='full')\n",
    "pca_skl.fit(standardized.T)\n",
    "print(pca_skl.get_covariance())\n",
    "print(pca.unstandardize_sample(pca_skl.inverse_transform(pca_skl.transform(pca.standardize_sample(x).reshape(1,-1)))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5943d4bc",
   "metadata": {},
   "source": [
    "## Analysis of Triathlon Data Set\n",
    "\n",
    "We have obtained a data set from https://www.kaggle.com/mpwolke/wired-differently-triathlon/data. This data represents results from an Ironman 70.3 triathlon race that took place in 2019. We want to perform PCA analysis on the data set and compare PCA dimension reduction with SVD dimension reduction. We also want to explore the correlations between each of the sub-events (swim, bike, run) as well as overall time among the race participants.\n",
    "\n",
    "### PCA and SVD Dimension Reduction\n",
    "\n",
    "We will start by performing PCA and SVD dimension reduction on each sample in our data set. We will vary the number of dimensions and observe the accuracy of each method. We use the SVD implementation provided by the SciKit Learn library, consistent with the SVD algorithm outlined in section 10.4 of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde7422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "779c798a75e127e3aa96660aebf9b74d3e571412428da99918dfc4cadf485d44"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
