{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Mini-project 1: Principal Component Analysis (PCA)\n",
    "#### Author: Jace Kline\n",
    "\n",
    "## Algorithm Development Steps\n",
    "\n",
    "In this section, we walk through the steps behind implementing the PCA algorithm outlined in section 10.6 of the textbook. The algorithm will consist of four main steps:\n",
    "\n",
    "1. Centering the original data set samples around the origin\n",
    "2. Standardizing the data set based on standard deviation\n",
    "3. Computing the eigendecomposition of the covariance matrix from the centered, standardized data set\n",
    "4. Choosing the dimension for approximating the original space\n",
    "\n",
    "\n",
    "#### Assumptions\n",
    "\n",
    "* The data matrix X is structured such that rows are attributes and columns are samples\n",
    "* The number of rows in data matrix X is less than the number of columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.decomposition as skl\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use the following data matrix below throughout the development of the algorithm to demonstrate intermediate steps."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "source": [
    "# sample data matrix for testing/demonstration\n",
    "\n",
    "A = np.array([\n",
    "    [2,8,2,1,5],\n",
    "    [8,7,2,2,6],\n",
    "    [4,0,5,0,4]\n",
    "])\n",
    "\n",
    "print(A)\n",
    "print(A.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[2 8 2 1 5]\n",
      " [8 7 2 2 6]\n",
      " [4 0 5 0 4]]\n",
      "(3, 5)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Centering the Data Matrix\n",
    "\n",
    "Per the algorithm for PCA in section 10.6 of the book, we must first center the data matrix so that each dimension has a mean of 0. We achieve this by computing the mean of each dimension (row) and then subtracting all elements in each row by the respective row's mean value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "source": [
    "D, samples = A.shape\n",
    "rowmeans = np.mean(A, axis=1)\n",
    "offsetmatrix = np.repeat(rowmeans, samples, axis=0).reshape((D,samples))\n",
    "centered = A - offsetmatrix\n",
    "\n",
    "print(centered)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-1.6  4.4 -1.6 -2.6  1.4]\n",
      " [ 3.   2.  -3.  -3.   1. ]\n",
      " [ 1.4 -2.6  2.4 -2.6  1.4]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Standardization\n",
    "\n",
    "The next step in the PCA algorithm involves standardizing each component of the data matrix by dividing by the component's respective standard deviation. We show this behavior below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "source": [
    "rowstds = np.std(centered, axis=1)\n",
    "standardized = (centered.T / rowstds).T\n",
    "\n",
    "print(standardized)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-0.62092042  1.70753116 -0.62092042 -1.00899568  0.54330537]\n",
      " [ 1.18585412  0.79056942 -1.18585412 -1.18585412  0.39528471]\n",
      " [ 0.64993368 -1.2070197   1.11417203 -1.2070197   0.64993368]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Eigendecomposition of the Covariance Matrix\n",
    "\n",
    "We must first find the covariance matrix of the centered and standardized data array. We then compute the eigendecomposition of this covariance matrix. Following this, we can choose a desired number of dimensions to use for dimensionality reduction."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "source": [
    "covmatrix = np.cov(standardized)\n",
    "print(covmatrix)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 1.25        0.69030098 -0.39635072]\n",
      " [ 0.69030098  1.25        0.04587658]\n",
      " [-0.39635072  0.04587658  1.25      ]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "source": [
    "# Compute the eigenvalues and unit-length eigenvectors of the standardized covariance matrix\n",
    "# The eigenvalues and vectors are ordered in ascending order\n",
    "\n",
    "res = np.linalg.eigh(covmatrix)\n",
    "\n",
    "# flip the order so the eigenvalues and vectors are sorted in descending order based on eigenvalue\n",
    "eigvals = np.flip(res[0])\n",
    "eigvecs = np.flip(res[1], axis=1)\n",
    "\n",
    "print(eigvals)\n",
    "print(eigvecs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2.026786  1.2895867 0.4336273]\n",
      "[[ 0.71551818 -0.02918716 -0.69798413]\n",
      " [ 0.61644272  0.4964592   0.61116826]\n",
      " [-0.32868237  0.86756923 -0.3732178 ]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We show below how we execute dimensionality reduction. We simply choose the same number of eigenvectors (descending order by weight) as the number dimensions we desire, and we represent these in a matrix as column vectors. We call this matrix B."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "source": [
    "DESIRED_DIMENSIONS = 2\n",
    "\n",
    "B = eigvecs[:, 0:DESIRED_DIMENSIONS]\n",
    "print(B)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.71551818 -0.02918716]\n",
      " [ 0.61644272  0.4964592 ]\n",
      " [-0.32868237  0.86756923]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4: Projection\n",
    "\n",
    "Using our dimension-reducing matrix, we can take vectors from the original space and project them onto the principal subspace with fewer dimensions than the original space. To express this projection in the original space, we multiply by the original standard deviation and add the mean of each for each vector component."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "source": [
    "# x = a vector from the original space\n",
    "\n",
    "x = np.array([1,1,1])\n",
    "\n",
    "\n",
    "# x_standardized = x tranformed into centered, standardized space\n",
    "\n",
    "x_standardized = (x - rowmeans) / rowstds\n",
    "\n",
    "\n",
    "# x_principal = the vector obtained by transforming x_standardized into the principal subspace\n",
    "\n",
    "x_principal = B @ (B.T @ x_standardized)\n",
    "\n",
    "\n",
    "# x_approx = x_principal transformed back into the original space ~> An approximation of x after dimension reduction\n",
    "\n",
    "x_approx = (x_principal * rowstds) + rowmeans\n",
    "\n",
    "print(\"x =\", x)\n",
    "print(\"x_standardized =\", x_standardized)\n",
    "print(\"x_principal =\", x_principal)\n",
    "print(\"x_approx =\", x_approx)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x = [1 1 1]\n",
      "x_standardized = [-1.00899568 -1.58113883 -0.74278135]\n",
      "x_principal = [-0.99842797 -1.59039212 -0.73713071]\n",
      "x_approx = [1.02723108 0.97659083 1.01217185]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that in this example the approximation is quite close to the original sample after dimension reduction. \n",
    "\n",
    "## Algorithm Implementation\n",
    "\n",
    "Now that we have illustrated the steps for the algorithm, we show the implementation of this procedure as a Python class."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "source": [
    "# Python class to express the behavior of PCA analysis\n",
    "class PCA:\n",
    "    # initialize the PCA class with a given data set X (required)\n",
    "    # optionally supply N, the number of reduction dimensions\n",
    "    def __init__(self, X, M=1):\n",
    "        # set the class variables\n",
    "        self.X = X\n",
    "        self.M = M\n",
    "        D, samples = self.X.shape\n",
    "        self.rowmeans = np.mean(X, axis=1)\n",
    "        self.rowstds = np.std(self.X, axis=1)\n",
    "\n",
    "        # standardize the centered data matrix\n",
    "        self.standardized = self.standardize(X)\n",
    "\n",
    "        # compute the covariance matrix\n",
    "        self.covmatrix = np.cov(self.standardized)\n",
    "\n",
    "        # compute the eigendecomposition of the covariance matrix\n",
    "        res = np.linalg.eigh(self.covmatrix)\n",
    "        self.eigvals = np.flip(res[0])\n",
    "        self.eigvecs = np.flip(res[1], axis=1)\n",
    "\n",
    "        # compute B\n",
    "        self.B = self.eigvecs[:, 0:self.M]\n",
    "\n",
    "    # set N, the number of dimensions to reduce to\n",
    "    def set_M(self, M):\n",
    "        # set N, recompute B\n",
    "        self.M = M\n",
    "        self.B = self.eigvecs[:, 0:self.M]\n",
    "\n",
    "    # center and standardize variance to 1 of sample(s)\n",
    "    def standardize(self, x):\n",
    "        if len(x.shape) == 1: # 1d array\n",
    "            return (x - self.rowmeans) / self.rowstds\n",
    "        else:\n",
    "            D, samples = x.shape\n",
    "            centered = x - np.repeat(self.rowmeans, samples, axis=0).reshape((D,samples))\n",
    "            standardized = (centered.T / self.rowstds).T\n",
    "            return standardized\n",
    "\n",
    "    # shift sample(s) back to original data space\n",
    "    def unstandardize(self, x):\n",
    "        if len(x.shape) == 1:\n",
    "            return (x * self.rowstds) + self.rowmeans\n",
    "        else:\n",
    "            D, samples = x.shape\n",
    "            return (x.T * self.rowstds).T + np.repeat(self.rowmeans, samples, axis=0).reshape((D,samples))\n",
    "\n",
    "    # return the covariance matrix of the centered, standardized data\n",
    "    def get_covariance_matrix(self):\n",
    "        return self.covmatrix\n",
    "\n",
    "    # transform a standardized sample of D dimensions into N dimensions\n",
    "    def transform_reduce(self, x):\n",
    "        return self.B.T @ x\n",
    "\n",
    "    # transform a dimension-reduced sample of N dimensions into D dimensions\n",
    "    # the result is centered and standardized\n",
    "    def transform_inverse(self, z):\n",
    "        return self.B @ z\n",
    "\n",
    "    # perform end-to-end transformation\n",
    "    # centers, standardizes, reduces, inverts, and unstandardizes\n",
    "    # this function takes a sample and \"approximates\" it using PCA with given N\n",
    "    def transform(self, x):\n",
    "        x_standardized = self.standardize(x)\n",
    "        x_principal = self.transform_inverse(self.transform_reduce(x_standardized))\n",
    "        x_transformed = self.unstandardize(x_principal)\n",
    "        return x_transformed\n",
    "\n",
    "    # perform dimension reduction on the entire X data set\n",
    "    # then map back to original X space\n",
    "    def pca_reduce_dataset(self, M=-1):\n",
    "        if M > 0:\n",
    "            self.set_M(M)\n",
    "        return self.unstandardize(self.B @ self.B.T @ self.standardized)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementation Testing\n",
    "\n",
    "Now that we have an implementation, we can test this implementation against the SciKit Learn implementation of PCA to ensure we get the same result on a test sample. The difference between our implementation and the SciKit Learn implementation is that our implementation automatically centers and standardizes the dataset prior to calculating the covariance matrix and eigendecompositions. Hence, we must have SciKit Learn's implementation perform PCA on the standardized data set, and we also must perform standardization on the sample point and undo this standardization after performing transformation. Below, we show the results of dimension reduction against sample x=(1,1,1) defined above. We see that both implementations compute the same covariance matrix and transformed sample."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "source": [
    "# Our implementation of PCA dimension reduction\n",
    "\n",
    "pca = PCA(A, M=DESIRED_DIMENSIONS)\n",
    "print(pca.get_covariance_matrix())\n",
    "print(pca.transform(x))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 1.25        0.69030098 -0.39635072]\n",
      " [ 0.69030098  1.25        0.04587658]\n",
      " [-0.39635072  0.04587658  1.25      ]]\n",
      "[1.02723108 0.97659083 1.01217185]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "source": [
    "# SciKit Learn implementation of PCA dimension reduction\n",
    "# This implementation does not automatically center and standardize the data set\n",
    "# Hence, we must perform the PCA on the centered, standardized dataset\n",
    "\n",
    "pca_skl = skl.PCA(n_components=DESIRED_DIMENSIONS, svd_solver='full')\n",
    "pca_skl.fit(standardized.T)\n",
    "print(pca_skl.get_covariance())\n",
    "print(pca.unstandardize(pca_skl.inverse_transform(pca_skl.transform(pca.standardize(x).reshape(1,-1))).reshape(3,)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 1.25        0.69030098 -0.39635072]\n",
      " [ 0.69030098  1.25        0.04587658]\n",
      " [-0.39635072  0.04587658  1.25      ]]\n",
      "[1.02723108 0.97659083 1.01217185]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analysis of Triathlon Data Set\n",
    "\n",
    "We have obtained a data set from https://www.kaggle.com/mpwolke/wired-differently-triathlon/data. This data represents results from an Ironman 70.3 triathlon race that took place in 2019. We want to perform PCA analysis on the data set and compare PCA dimension reduction with SVD dimension reduction. We also want to explore the correlations between each of the sub-events (swim, bike, run) as well as overall time among the race participants.\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "Prior to performing analysis, we first clean the data set into something useable in PCA dimension reduction. We ensure that all features of the data set are numerical and we remove unnecessary features not applicable to our analysis. Below we show the original data set compared to the cleaned data set. The data cleaning process can be found [here](./clean.ipynb)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "source": [
    "# show the original data set\n",
    "print(pd.read_csv(\"../data/original.csv\").head())\n",
    "\n",
    "# store the cleaned data set into a dataframe and print it\n",
    "df = pd.read_csv(\"../data/cleaned.csv\")\n",
    "print(df.head())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Unnamed: 0  Pos                       Name                     Club  \\\n",
      "0           1    1            FRODENO Jan (2)        Laz  Saarbruecken   \n",
      "1           2    2        CLAVEL Maurice (24)                      NaN   \n",
      "2           3    3  TAAGHOLT Miki  Morck (21)                    Ttsdu   \n",
      "3           4    4         STRATMANN Jan (13)  Triathlon  Team  Witten   \n",
      "4           5    5       MOLINARI Giulio (22)        C. S. Carabinieri   \n",
      "\n",
      "        Cat        Swim          T1       Bike          T2         Run  \\\n",
      "0  MPRO - 1   00:22:501  00:01:2710  02:02:011  00:01:5221   01:11:252   \n",
      "1  MPRO - 2  00:23:3311  00:01:3015  02:04:543   00:01:323   01:12:144   \n",
      "2  MPRO - 3   00:22:574   00:01:141  02:05:526  00:02:0148   01:14:238   \n",
      "3  MPRO - 4   00:22:502  00:01:3118  02:08:208  00:01:5633   01:13:336   \n",
      "4  MPRO - 5   00:22:585  00:01:4968  02:05:114  00:02:0768  01:17:2117   \n",
      "\n",
      "       Time  \n",
      "0  03:39:35  \n",
      "1  03:43:43  \n",
      "2  03:46:27  \n",
      "3  03:48:10  \n",
      "4  03:49:26  \n",
      "   Swim   T1  Bike   T2   Run   Time  Gender  AgeGroup\n",
      "0  1370   87  7321  112  4285  13175       1         9\n",
      "1  1413   90  7494   92  4334  13423       1         9\n",
      "2  1377   74  7552  121  4463  13587       1         9\n",
      "3  1370   91  7700  116  4413  13690       1         9\n",
      "4  1378  109  7511  127  4641  13766       1         9\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PCA and SVD Dimension Reduction\n",
    "\n",
    "We will start by performing PCA and SVD dimension reduction on each sample in our data set. We will vary the number of dimensions and observe the accuracy of each method. We use the SVD implementation provided by the SciKit Learn library, consistent with the SVD algorithm outlined in section 10.4 of the book."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "source": [
    "# store the data set as an array\n",
    "# features are rows, samples are columns\n",
    "X = df.to_numpy().T\n",
    "X_features, X_samples = X.shape\n",
    "\n",
    "print(\"Data matrix:\\n\", X)\n",
    "print(\"\\nDimensions:\", X.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data matrix:\n",
      " [[ 1370  1413  1377 ...  2481  2436  2230]\n",
      " [   87    90    74 ...   338   128   278]\n",
      " [ 7321  7494  7552 ...  9599  9676  9702]\n",
      " ...\n",
      " [13175 13423 13587 ... 19279 19286 19286]\n",
      " [    1     1     1 ...     1     1     1]\n",
      " [    9     9     9 ...     3     4     4]]\n",
      "\n",
      "Dimensions: (8, 797)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "source": [
    "# Create SVD decomposition of X\n",
    "U, singular_values, Vt = np.linalg.svd(X, full_matrices=False, compute_uv=True)\n",
    "Sigma = np.diag(singular_values)\n",
    "\n",
    "# Create PCA decomposition of X\n",
    "pca = PCA(X)\n",
    "\n",
    "# define a function that computes the SVD approximation of X in N dimensions\n",
    "def svd_reduce_dataset(M):\n",
    "    return U[:,0:M] @ Sigma[0:M,0:M] @ Vt[0:M,:]\n",
    "\n",
    "# define a function that computes the PCA approximation of X in N dimensions\n",
    "def pca_reduce_dataset(M):\n",
    "    return pca.pca_reduce_dataset(M=M)\n",
    "\n",
    "# define a function that measures the addition of the sum of squares error for each sample between the original and approximate data matrix\n",
    "def error_sum_of_squares(X_approx):\n",
    "    return np.sum(np.sqrt(np.sum(np.square(X - X_approx), axis=0)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "source": [
    "# Loop over possible dimensions to reduce to and perform both SVD and PCA dimension reduction on X\n",
    "# Compute the sum of squares value \n",
    "for M in range(1, X_features):\n",
    "    svd_reduced = svd_reduce_dataset(M)\n",
    "    pca_reduced = pca_reduce_dataset(M)\n",
    "    \n",
    "    svd_error = error_sum_of_squares(svd_reduced)\n",
    "    pca_error = error_sum_of_squares(pca_reduced)\n",
    "\n",
    "    print(f\"M = {M}: SVD error = {svd_error}; PCA error = {pca_error}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "M = 1: SVD error = 315458.2170843122; PCA error = 408002.3055187013\n",
      "M = 2: SVD error = 158226.04625871772; PCA error = 370857.82630964444\n",
      "M = 3: SVD error = 47832.966943956155; PCA error = 366832.5805188172\n",
      "M = 4: SVD error = 21862.17064038208; PCA error = 297928.13345273404\n",
      "M = 5: SVD error = 1254.2185559626928; PCA error = 232482.10173219253\n",
      "M = 6: SVD error = 222.92900976507406; PCA error = 195219.7247403326\n",
      "M = 7: SVD error = 154.71946657773168; PCA error = 269.48678760111204\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analysis of SVD vs PCA\n",
    "\n",
    "With our given data set, it is evident by measuring the sum of the \"distance\" measures between the reduced matrices and the original matrix results in more accurate modeling by the SVD dimension reduction method over PCA. This can be explained through observation of the singular values and eigenvalues of the SVD and PCA methods, respectively. We see below that the singular values in the SVD decomposition have much greater percent differential between subsequent values, implying that the first few dimensions carry more \"weight\" in the decomposition, and therefore will keep the dimension reduction fairly accurate for reduced values of M. On the contrary, the PCA eigenvalues show relatively similar magnitudes, leading to the conclusion that the PCA is not finding highly weighted principals and therefore the dimension reduction approximation will suffer overall, particularly for lower values of M."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "source": [
    "print(\"SVD singular values:\\n\", singular_values)\n",
    "print(\"\\nWeights of SVD singular values:\\n\", singular_values / np.sum(singular_values))\n",
    "\n",
    "print(\"\\nPCA eigenvalues:\\n\", pca.eigvals)\n",
    "print(\"\\nWeights of PCA eigenvalues:\\n\", pca.eigvals / np.sum(pca.eigvals))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SVD singular values:\n",
      " [588557.15869969  10892.83534843   6469.0560675    1812.64860764\n",
      "   1106.68531687     56.92761533      7.63188929      5.62563419]\n",
      "\n",
      "Weights of SVD singular values:\n",
      " [0.96657723 0.01788911 0.01062402 0.00297688 0.00181749 0.00009349\n",
      " 0.00001253 0.00000924]\n",
      "\n",
      "PCA eigenvalues:\n",
      " [3.75212819 1.12778623 0.94059361 0.73174192 0.63925611 0.47904664\n",
      " 0.33949745 0.00000011]\n",
      "\n",
      "Weights of PCA eigenvalues:\n",
      " [0.46842755 0.1407964  0.11742668 0.09135297 0.07980675 0.0598057\n",
      " 0.04238393 0.00000001]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "779c798a75e127e3aa96660aebf9b74d3e571412428da99918dfc4cadf485d44"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}