{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Mini-project 1: Principal Component Analysis (PCA)\n",
    "#### Author: Jace Kline\n",
    "\n",
    "## Algorithm Development Steps\n",
    "\n",
    "In this section, we walk through the steps behind implementing the PCA algorithm outlined in section 10.6 of the textbook. The algorithm will consist of four main steps:\n",
    "\n",
    "1. Centering the original data set samples around the origin\n",
    "2. Standardizing the data set based on standard deviation\n",
    "3. Computing the eigendecomposition of the covariance matrix from the centered, standardized data set\n",
    "4. Choosing the dimension for approximating the original space\n",
    "\n",
    "\n",
    "#### Assumptions\n",
    "\n",
    "* The data matrix X is structured such that rows are attributes and columns are samples\n",
    "* The number of rows in data matrix X is less than the number of columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.decomposition as skl\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use the following data matrix below throughout the development of the algorithm to demonstrate intermediate steps."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "source": [
    "# sample data matrix for testing/demonstration\n",
    "\n",
    "A = np.array([\n",
    "    [2,8,2,1,5],\n",
    "    [8,7,2,2,6],\n",
    "    [4,0,5,0,4]\n",
    "])\n",
    "\n",
    "print(\"A:\\n\", A)\n",
    "print(\"A dimensions:\", A.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A:\n",
      " [[2 8 2 1 5]\n",
      " [8 7 2 2 6]\n",
      " [4 0 5 0 4]]\n",
      "A dimensions: (3, 5)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Centering the Data Matrix\n",
    "\n",
    "Per the algorithm for PCA in section 10.6 of the book, we must first center the data matrix so that each dimension has a mean of 0. We achieve this by computing the mean of each dimension (row) and then subtracting all elements in each row by the respective row's mean value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "source": [
    "D, samples = A.shape\n",
    "rowmeans = np.mean(A, axis=1)\n",
    "offsetmatrix = np.repeat(rowmeans, samples, axis=0).reshape((D,samples))\n",
    "centered = A - offsetmatrix\n",
    "\n",
    "print(\"Centered sample data set:\\n\", centered)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Centered sample data set:\n",
      " [[-1.6  4.4 -1.6 -2.6  1.4]\n",
      " [ 3.   2.  -3.  -3.   1. ]\n",
      " [ 1.4 -2.6  2.4 -2.6  1.4]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Standardization\n",
    "\n",
    "The next step in the PCA algorithm involves standardizing each component of the data matrix by dividing by the component's respective standard deviation. We show this behavior below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "source": [
    "rowstds = np.std(centered, axis=1)\n",
    "standardized = (centered.T / rowstds).T\n",
    "\n",
    "print(\"Centered and standardized sample data set:\\n\", standardized)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Centered and standardized sample data set:\n",
      " [[-0.62092   1.707531 -0.62092  -1.008996  0.543305]\n",
      " [ 1.185854  0.790569 -1.185854 -1.185854  0.395285]\n",
      " [ 0.649934 -1.20702   1.114172 -1.20702   0.649934]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Eigendecomposition of the Covariance Matrix\n",
    "\n",
    "We must first find the covariance matrix of the centered and standardized data array. We then compute the eigendecomposition of this covariance matrix. Following this, we can choose a desired number of dimensions to use for dimensionality reduction."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "source": [
    "covmatrix = np.cov(standardized)\n",
    "print(\"Covariance matrix:\\n\", covmatrix)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Covariance matrix:\n",
      " [[ 1.25      0.690301 -0.396351]\n",
      " [ 0.690301  1.25      0.045877]\n",
      " [-0.396351  0.045877  1.25    ]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "source": [
    "# Compute the eigenvalues and unit-length eigenvectors of the standardized covariance matrix\n",
    "# The eigenvalues and vectors are ordered in ascending order\n",
    "\n",
    "res = np.linalg.eigh(covmatrix)\n",
    "\n",
    "# flip the order so the eigenvalues and vectors are sorted in descending order based on eigenvalue\n",
    "eigvals = np.flip(res[0])\n",
    "eigvecs = np.flip(res[1], axis=1)\n",
    "\n",
    "print(\"Eigenvalues:\\n\", eigvals)\n",
    "print(\"Eigenvectors:\\n\", eigvecs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Eigenvalues:\n",
      " [2.026786 1.289587 0.433627]\n",
      "Eigenvectors:\n",
      " [[ 0.715518 -0.029187 -0.697984]\n",
      " [ 0.616443  0.496459  0.611168]\n",
      " [-0.328682  0.867569 -0.373218]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We show below how we execute dimensionality reduction. We simply choose the same number of eigenvectors (descending order by weight) as the number dimensions we desire, and we represent these in a matrix as column vectors. We call this matrix B."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "source": [
    "DESIRED_DIMENSIONS = 2\n",
    "\n",
    "B = eigvecs[:, 0:DESIRED_DIMENSIONS]\n",
    "print(B)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.715518 -0.029187]\n",
      " [ 0.616443  0.496459]\n",
      " [-0.328682  0.867569]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4: Projection\n",
    "\n",
    "Using our dimension-reducing matrix, we can take vectors from the original space and project them onto the principal subspace with fewer dimensions than the original space. To express this projection in the original space, we multiply by the original standard deviation and add the mean of each for each vector component."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "source": [
    "# x = a vector from the original space\n",
    "\n",
    "x = np.array([1,1,1])\n",
    "\n",
    "\n",
    "# x_standardized = x tranformed into centered, standardized space\n",
    "\n",
    "x_standardized = (x - rowmeans) / rowstds\n",
    "\n",
    "\n",
    "# x_principal = the vector obtained by transforming x_standardized into the principal subspace\n",
    "\n",
    "x_principal = B @ (B.T @ x_standardized)\n",
    "\n",
    "\n",
    "# x_approx = x_principal transformed back into the original space ~> An approximation of x after dimension reduction\n",
    "\n",
    "x_approx = (x_principal * rowstds) + rowmeans\n",
    "\n",
    "print(\"x =\", x)\n",
    "print(\"x_standardized =\", x_standardized)\n",
    "print(\"x_principal =\", x_principal)\n",
    "print(\"x_approx =\", x_approx)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x = [1 1 1]\n",
      "x_standardized = [-1.008996 -1.581139 -0.742781]\n",
      "x_principal = [-0.998428 -1.590392 -0.737131]\n",
      "x_approx = [1.027231 0.976591 1.012172]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that in this example the approximation is quite close to the original sample after dimension reduction. \n",
    "\n",
    "## Algorithm Implementation\n",
    "\n",
    "Now that we have illustrated the steps for the algorithm, we show the implementation of this procedure as a Python class."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "source": [
    "# Python class to express the behavior of PCA analysis\n",
    "class PCA:\n",
    "    # initialize the PCA class with a given data set X (required)\n",
    "    # optionally supply N, the number of reduction dimensions\n",
    "    def __init__(self, X, M=1):\n",
    "        # set the class variables\n",
    "        self.X = X\n",
    "        self.M = M\n",
    "        D, samples = self.X.shape\n",
    "        self.rowmeans = np.mean(X, axis=1)\n",
    "        self.rowstds = np.std(self.X, axis=1)\n",
    "\n",
    "        # standardize the centered data matrix\n",
    "        self.standardized = self.standardize(X)\n",
    "\n",
    "        # compute the covariance matrix\n",
    "        self.covmatrix = np.cov(self.standardized)\n",
    "\n",
    "        # compute the eigendecomposition of the covariance matrix\n",
    "        res = np.linalg.eigh(self.covmatrix)\n",
    "        self.eigvals = np.flip(res[0])\n",
    "        self.eigvecs = np.flip(res[1], axis=1)\n",
    "\n",
    "        # compute B\n",
    "        self.B = self.eigvecs[:, 0:self.M]\n",
    "\n",
    "    # set N, the number of dimensions to reduce to\n",
    "    def set_M(self, M):\n",
    "        # set N, recompute B\n",
    "        self.M = M\n",
    "        self.B = self.eigvecs[:, 0:self.M]\n",
    "\n",
    "    # center and standardize variance to 1 of sample(s)\n",
    "    def standardize(self, x):\n",
    "        if len(x.shape) == 1: # 1d array\n",
    "            return (x - self.rowmeans) / self.rowstds\n",
    "        else:\n",
    "            D, samples = x.shape\n",
    "            centered = x - np.repeat(self.rowmeans, samples, axis=0).reshape((D,samples))\n",
    "            standardized = (centered.T / self.rowstds).T\n",
    "            return standardized\n",
    "\n",
    "    # shift sample(s) back to original data space\n",
    "    def unstandardize(self, x):\n",
    "        if len(x.shape) == 1:\n",
    "            return (x * self.rowstds) + self.rowmeans\n",
    "        else:\n",
    "            D, samples = x.shape\n",
    "            return (x.T * self.rowstds).T + np.repeat(self.rowmeans, samples, axis=0).reshape((D,samples))\n",
    "\n",
    "    # return the covariance matrix of the centered, standardized data\n",
    "    def get_covariance_matrix(self):\n",
    "        return self.covmatrix\n",
    "\n",
    "    # transform a standardized sample of D dimensions into N dimensions\n",
    "    def transform_reduce(self, x):\n",
    "        return self.B.T @ x\n",
    "\n",
    "    # transform a dimension-reduced sample of N dimensions into D dimensions\n",
    "    # the result is centered and standardized\n",
    "    def transform_inverse(self, z):\n",
    "        return self.B @ z\n",
    "\n",
    "    # perform end-to-end transformation\n",
    "    # centers, standardizes, reduces, inverts, and unstandardizes\n",
    "    # this function takes a sample and \"approximates\" it using PCA with given N\n",
    "    def transform(self, x):\n",
    "        x_standardized = self.standardize(x)\n",
    "        x_principal = self.transform_inverse(self.transform_reduce(x_standardized))\n",
    "        x_transformed = self.unstandardize(x_principal)\n",
    "        return x_transformed\n",
    "\n",
    "    # perform dimension reduction on the entire X data set\n",
    "    # then map back to original X space\n",
    "    def pca_reduce_dataset(self, M=-1):\n",
    "        if M > 0:\n",
    "            self.set_M(M)\n",
    "        return self.unstandardize(self.B @ self.B.T @ self.standardized)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementation Testing\n",
    "\n",
    "Now that we have an implementation, we can test this implementation against the SciKit Learn implementation of PCA to ensure we get the same result on a test sample. The difference between our implementation and the SciKit Learn implementation is that our implementation automatically centers and standardizes the dataset prior to calculating the covariance matrix and eigendecompositions. Hence, we must have SciKit Learn's implementation perform PCA on the standardized data set, and we also must perform standardization on the sample point and undo this standardization after performing transformation. Below, we show the results of dimension reduction against sample x=(1,1,1) defined above. We see that both implementations compute the same covariance matrix and transformed sample."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "source": [
    "# Our implementation of PCA dimension reduction\n",
    "\n",
    "pca = PCA(A, M=DESIRED_DIMENSIONS)\n",
    "print(\"Covariance matrix:\\n\", pca.get_covariance_matrix())\n",
    "print(\"Dimension-reduced vector x:\\n\", pca.transform(x))\n",
    "print(\"Weights of eigenvalues (explained variance ratio):\\n\", pca.eigvals / np.sum(pca.eigvals))\n",
    "print(\"Cumulative weights of eigenvalues (cumulative explained variance ratio):\\n\", np.cumsum(pca.eigvals / np.sum(pca.eigvals)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Covariance matrix:\n",
      " [[ 1.25      0.690301 -0.396351]\n",
      " [ 0.690301  1.25      0.045877]\n",
      " [-0.396351  0.045877  1.25    ]]\n",
      "Dimension-reduced vector x:\n",
      " [1.027231 0.976591 1.012172]\n",
      "Weights of eigenvalues (explained variance ratio):\n",
      " [0.540476 0.34389  0.115634]\n",
      "Cumulative weights of eigenvalues (cumulative explained variance ratio):\n",
      " [0.540476 0.884366 1.      ]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "source": [
    "# SciKit Learn implementation of PCA dimension reduction\n",
    "# This implementation does not automatically center and standardize the data set\n",
    "# Hence, we must perform the PCA on the centered, standardized dataset\n",
    "\n",
    "pca_skl = skl.PCA(n_components=DESIRED_DIMENSIONS, svd_solver='full')\n",
    "pca_skl.fit(standardized.T)\n",
    "print(\"Covariance matrix:\\n\", pca_skl.get_covariance())\n",
    "print(\"Dimension-reduced vector x:\\n\", pca.unstandardize(pca_skl.inverse_transform(pca_skl.transform(pca.standardize(x).reshape(1,-1))).reshape(3,)))\n",
    "\n",
    "pca_skl.set_params(n_components=D)\n",
    "print(\"Weights of eigenvalues (explained variance ratio):\\n\", pca_skl.explained_variance_ratio_)\n",
    "print(\"Cumulative weights of eigenvalues (cumulative explained variance ratio):\\n\", np.cumsum(pca_skl.explained_variance_ratio_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Covariance matrix:\n",
      " [[ 1.25      0.690301 -0.396351]\n",
      " [ 0.690301  1.25      0.045877]\n",
      " [-0.396351  0.045877  1.25    ]]\n",
      "Dimension-reduced vector x:\n",
      " [1.027231 0.976591 1.012172]\n",
      "Weights of eigenvalues (explained variance ratio):\n",
      " [0.540476 0.34389 ]\n",
      "Cumulative weights of eigenvalues (cumulative explained variance ratio):\n",
      " [0.540476 0.884366]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analysis of Triathlon Data Set\n",
    "\n",
    "We have obtained a data set from https://www.kaggle.com/mpwolke/wired-differently-triathlon/data. This data represents results from an Ironman 70.3 triathlon race that took place in 2019. We want to perform PCA analysis on the data set and compare PCA dimension reduction with SVD dimension reduction. We also want to explore the correlations between each of the sub-events (swim, bike, run) as well as overall time among the race participants.\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "Prior to performing analysis, we first clean the data set into something useable in PCA dimension reduction. We ensure that all features of the data set are numerical and we remove unnecessary features not applicable to our analysis. For the time features, we simply convert these times into a number of seconds. The gender feature is codified as female=0, male=1. The age group is sorted in ascending order and given an index starting with the 18-24 year-old group at value 0, followed by the 25-29 year-olds at value 1, and continuing in this pattern until concluding with the pro division at value 9. Below we show the original data set compared to the cleaned data set. The data cleaning process can be found [here](https://github.com/jace-kline/math582-miniproject1/blob/main/reports/clean.md)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "source": [
    "# show the original data set\n",
    "print(\"Original data:\\n\", pd.read_csv(\"../data/original.csv\").head())\n",
    "\n",
    "# store the cleaned data set into a dataframe and print it\n",
    "df = pd.read_csv(\"../data/cleaned.csv\")\n",
    "print(\"\\nCleaned data:\\n\", df.head())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original data:\n",
      "    Unnamed: 0  Pos                       Name                     Club  \\\n",
      "0           1    1            FRODENO Jan (2)        Laz  Saarbruecken   \n",
      "1           2    2        CLAVEL Maurice (24)                      NaN   \n",
      "2           3    3  TAAGHOLT Miki  Morck (21)                    Ttsdu   \n",
      "3           4    4         STRATMANN Jan (13)  Triathlon  Team  Witten   \n",
      "4           5    5       MOLINARI Giulio (22)        C. S. Carabinieri   \n",
      "\n",
      "        Cat        Swim          T1       Bike          T2         Run  \\\n",
      "0  MPRO - 1   00:22:501  00:01:2710  02:02:011  00:01:5221   01:11:252   \n",
      "1  MPRO - 2  00:23:3311  00:01:3015  02:04:543   00:01:323   01:12:144   \n",
      "2  MPRO - 3   00:22:574   00:01:141  02:05:526  00:02:0148   01:14:238   \n",
      "3  MPRO - 4   00:22:502  00:01:3118  02:08:208  00:01:5633   01:13:336   \n",
      "4  MPRO - 5   00:22:585  00:01:4968  02:05:114  00:02:0768  01:17:2117   \n",
      "\n",
      "       Time  \n",
      "0  03:39:35  \n",
      "1  03:43:43  \n",
      "2  03:46:27  \n",
      "3  03:48:10  \n",
      "4  03:49:26  \n",
      "\n",
      "Cleaned data:\n",
      "    Swim   T1  Bike   T2   Run   Time  Gender  AgeGroup\n",
      "0  1370   87  7321  112  4285  13175       1         9\n",
      "1  1413   90  7494   92  4334  13423       1         9\n",
      "2  1377   74  7552  121  4463  13587       1         9\n",
      "3  1370   91  7700  116  4413  13690       1         9\n",
      "4  1378  109  7511  127  4641  13766       1         9\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PCA and SVD Dimension Reduction\n",
    "\n",
    "We will start by performing PCA and SVD dimension reduction on each sample in our data set. We will vary the number of dimensions and observe the accuracy of each method. We use the SVD implementation provided by the SciKit Learn library, consistent with the SVD algorithm outlined in section 10.4 of the book."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "source": [
    "# store the data set as an array\n",
    "# features are rows, samples are columns\n",
    "X = df.to_numpy().T\n",
    "X_features, X_samples = X.shape\n",
    "\n",
    "print(\"Data matrix:\\n\", X)\n",
    "print(\"\\nDimensions:\", X.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data matrix:\n",
      " [[ 1370  1413  1377 ...  2481  2436  2230]\n",
      " [   87    90    74 ...   338   128   278]\n",
      " [ 7321  7494  7552 ...  9599  9676  9702]\n",
      " ...\n",
      " [13175 13423 13587 ... 19279 19286 19286]\n",
      " [    1     1     1 ...     1     1     1]\n",
      " [    9     9     9 ...     3     4     4]]\n",
      "\n",
      "Dimensions: (8, 797)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "source": [
    "# Create SVD decomposition of X\n",
    "U, singular_values, Vt = np.linalg.svd(X, full_matrices=False, compute_uv=True)\n",
    "Sigma = np.diag(singular_values)\n",
    "\n",
    "# Create PCA decomposition of X\n",
    "pca = PCA(X)\n",
    "\n",
    "# define a function that computes the SVD approximation of X in N dimensions\n",
    "def svd_reduce_dataset(M):\n",
    "    return U[:,0:M] @ Sigma[0:M,0:M] @ Vt[0:M,:]\n",
    "\n",
    "# define a function that computes the PCA approximation of X in N dimensions\n",
    "def pca_reduce_dataset(M):\n",
    "    return pca.pca_reduce_dataset(M=M)\n",
    "\n",
    "# used to compute the \"error\" in our dimension reduction approximations\n",
    "def frobenius_norm(A):\n",
    "    return np.linalg.norm(A)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "source": [
    "# Loop over possible dimensions to reduce to and perform both SVD and PCA dimension reduction on X\n",
    "# Compute the sum of squares value \n",
    "for M in range(1, X_features):\n",
    "    svd_reduced = svd_reduce_dataset(M)\n",
    "    pca_reduced = pca_reduce_dataset(M)\n",
    "    \n",
    "    svd_error = frobenius_norm(X - svd_reduced)\n",
    "    pca_error = frobenius_norm(X - pca_reduced)\n",
    "\n",
    "    print(f\"M = {M}: SVD error = {svd_error}; PCA error = {pca_error}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "M = 1: SVD error = 12845.868065052797; PCA error = 16344.79537113727\n",
      "M = 2: SVD error = 6808.998782248694; PCA error = 14819.669507863737\n",
      "M = 3: SVD error = 2124.5653701961114; PCA error = 14711.263178927595\n",
      "M = 4: SVD error = 1108.1890801904326; PCA error = 12247.783107777936\n",
      "M = 5: SVD error = 57.71175687035737; PCA error = 9847.462906328448\n",
      "M = 6: SVD error = 9.481217969047252; PCA error = 8948.267230251893\n",
      "M = 7: SVD error = 5.625634185768421; PCA error = 9.762794765606992\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analysis of SVD vs PCA\n",
    "\n",
    "With our given data set, it is evident by computing the frobenius norm of the differences between the approximation matrices and the original matrix results in more accurate modeling by the SVD dimension reduction method over PCA. This can be explained through observation of the singular values and eigenvalues of the SVD and PCA methods, respectively. We see below that the singular values in the SVD decomposition have much greater percent differential between subsequent values, implying that the first few dimensions carry more \"weight\" in the decomposition, and therefore will keep the dimension reduction fairly accurate for reduced values of M. On the contrary, the PCA eigenvalues show relatively similar magnitudes, leading to the conclusion that the PCA is not finding highly weighted principal components and therefore the dimension reduction approximation will suffer overall, particularly for lower values of M. We show this below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "source": [
    "print(\"SVD singular values:\\n\", singular_values)\n",
    "print(\"Weights of SVD singular values:\\n\", singular_values / np.sum(singular_values))\n",
    "print(\"Cumulative weights of SVD singular values:\\n\", np.cumsum(singular_values / np.sum(singular_values)))\n",
    "\n",
    "print(\"\\nPCA eigenvalues:\\n\", pca.eigvals)\n",
    "print(\"Weights of PCA eigenvalues (explained variance ratio):\\n\", pca.eigvals / np.sum(pca.eigvals))\n",
    "print(\"Cumulative weights of PCA eigenvalues (cumulative explained variance ratio):\\n\", np.cumsum(pca.eigvals / np.sum(pca.eigvals)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SVD singular values:\n",
      " [588557.1587    10892.835348   6469.056067   1812.648608   1106.685317\n",
      "     56.927615      7.631889      5.625634]\n",
      "Weights of SVD singular values:\n",
      " [0.966577 0.017889 0.010624 0.002977 0.001817 0.000093 0.000013 0.000009]\n",
      "Cumulative weights of SVD singular values:\n",
      " [0.966577 0.984466 0.99509  0.998067 0.999885 0.999978 0.999991 1.      ]\n",
      "\n",
      "PCA eigenvalues:\n",
      " [3.752128 1.127786 0.940594 0.731742 0.639256 0.479047 0.339497 0.      ]\n",
      "Weights of PCA eigenvalues (explained variance ratio):\n",
      " [0.468428 0.140796 0.117427 0.091353 0.079807 0.059806 0.042384 0.      ]\n",
      "Cumulative weights of PCA eigenvalues (cumulative explained variance ratio):\n",
      " [0.468428 0.609224 0.726651 0.818004 0.89781  0.957616 1.       1.      ]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Error Tolerance of PCA Dimension Reduction\n",
    "\n",
    "We can use the cumulative explained variance ratio to determine how much variance we capture of our data when performing PCA dimension reduction with different values of M (reduction dimensions). If we are satisfied with capturing greater than 80% of the variance from the original data set, then we can use M>=4 as a dimension reduction approximation. However, if we want to capture greater than 95% of the variance from the original data set, then we must use M>=6."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Correlation Analysis\n",
    "\n",
    "Now that we have shown the comparision of SVD vs PCA for dimension reduction, we shift our focus to the correlations/covariances between the features in our data set. Particularly, we want to see which of the features are highest correlated with eachother and which are highest correlated with an athlete's overall finishing time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "source": [
    "df.corr()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Swim</th>\n",
       "      <th>T1</th>\n",
       "      <th>Bike</th>\n",
       "      <th>T2</th>\n",
       "      <th>Run</th>\n",
       "      <th>Time</th>\n",
       "      <th>Gender</th>\n",
       "      <th>AgeGroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Swim</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.477852</td>\n",
       "      <td>0.540819</td>\n",
       "      <td>0.313248</td>\n",
       "      <td>0.424090</td>\n",
       "      <td>0.691435</td>\n",
       "      <td>0.091466</td>\n",
       "      <td>-0.208273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1</th>\n",
       "      <td>0.477852</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.483319</td>\n",
       "      <td>0.428836</td>\n",
       "      <td>0.358845</td>\n",
       "      <td>0.553294</td>\n",
       "      <td>0.111193</td>\n",
       "      <td>-0.161320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bike</th>\n",
       "      <td>0.540819</td>\n",
       "      <td>0.483319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.359894</td>\n",
       "      <td>0.593732</td>\n",
       "      <td>0.895452</td>\n",
       "      <td>-0.133717</td>\n",
       "      <td>-0.235186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T2</th>\n",
       "      <td>0.313248</td>\n",
       "      <td>0.428836</td>\n",
       "      <td>0.359894</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.329472</td>\n",
       "      <td>0.446983</td>\n",
       "      <td>0.108297</td>\n",
       "      <td>-0.080900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run</th>\n",
       "      <td>0.424090</td>\n",
       "      <td>0.358845</td>\n",
       "      <td>0.593732</td>\n",
       "      <td>0.329472</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.852911</td>\n",
       "      <td>0.007329</td>\n",
       "      <td>-0.203262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <td>0.691435</td>\n",
       "      <td>0.553294</td>\n",
       "      <td>0.895452</td>\n",
       "      <td>0.446983</td>\n",
       "      <td>0.852911</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.032630</td>\n",
       "      <td>-0.257870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>0.091466</td>\n",
       "      <td>0.111193</td>\n",
       "      <td>-0.133717</td>\n",
       "      <td>0.108297</td>\n",
       "      <td>0.007329</td>\n",
       "      <td>-0.032630</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AgeGroup</th>\n",
       "      <td>-0.208273</td>\n",
       "      <td>-0.161320</td>\n",
       "      <td>-0.235186</td>\n",
       "      <td>-0.080900</td>\n",
       "      <td>-0.203262</td>\n",
       "      <td>-0.257870</td>\n",
       "      <td>-0.022239</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Swim        T1      Bike        T2       Run      Time  \\\n",
       "Swim      1.000000  0.477852  0.540819  0.313248  0.424090  0.691435   \n",
       "T1        0.477852  1.000000  0.483319  0.428836  0.358845  0.553294   \n",
       "Bike      0.540819  0.483319  1.000000  0.359894  0.593732  0.895452   \n",
       "T2        0.313248  0.428836  0.359894  1.000000  0.329472  0.446983   \n",
       "Run       0.424090  0.358845  0.593732  0.329472  1.000000  0.852911   \n",
       "Time      0.691435  0.553294  0.895452  0.446983  0.852911  1.000000   \n",
       "Gender    0.091466  0.111193 -0.133717  0.108297  0.007329 -0.032630   \n",
       "AgeGroup -0.208273 -0.161320 -0.235186 -0.080900 -0.203262 -0.257870   \n",
       "\n",
       "            Gender  AgeGroup  \n",
       "Swim      0.091466 -0.208273  \n",
       "T1        0.111193 -0.161320  \n",
       "Bike     -0.133717 -0.235186  \n",
       "T2        0.108297 -0.080900  \n",
       "Run       0.007329 -0.203262  \n",
       "Time     -0.032630 -0.257870  \n",
       "Gender    1.000000 -0.022239  \n",
       "AgeGroup -0.022239  1.000000  "
      ]
     },
     "metadata": {},
     "execution_count": 205
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the above correlation chart, we observe that the bike performance is highest correlated with an athlete's finishing time with correlation of 0.895. The run is second highest correlated with overall time at a value of 0.853. The swim portion has the lowest correlation value of the three sub-disciplines when compared to the overall time, coming in at 0.691. In addition, the transition times (T1, T2) are also intuitively positively correlated with overall finishing time, but the magnitude of these correlations are noticeably smaller. Interestingly, we also see that among the sub-disciplines, the bike and the run performances are the highest correlated at 0.594 with the swim-bike correlation following at 0.541 and the swim-run lowest correlated at 0.424. Another interesting finding from the correlations shows that age is much more linked to overall time than gender is.\n",
    "\n",
    "The key takeaway from this correlation analysis is that the bike portion of this distance of triathlon race is the most important when it comes to minimizing overall time, followed next by running time and then by swimming time. If one were to devise a training plan for an aspiring triathlete, this analysis shows that the most benefit to overall race performance should come from training for the bike, run, and swim in that order. A caveat to this is that this race distance (Ironman 70.3) has different race \"proportions\" for each of the sub-disciplines than other triathlon races. For instance, an Olympic distance triathlon has a higher percentage of the race distance allocated to the swim, which ultimately makes the swim more important in that race distance.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This report has been an attempt to detail the development and evaluation of the PCA algorithm outlined in section 10.6 of the textbook. The first part of this report walked through the thought process and reasoning behind constructing the PCA algorithm. The next part involved the implmentation of the PCA algorithm as a Python class. We tested our PCA implementation against the SciKit Learn PCA implementation and found the same results on a test data set and sample. Following this, we loaded our target data set containing a set of triathlete finishing times and sub-discipline times for an Ironman 70.3 triathlon race. We proceeded to perform SVD and PCA analysis on this data set, and we found that the SVD dimension reduction performed better than PCA on this particular data set. We explained that this was due to the variance captured in the eigenvalues of the covariance matrix being \"spread out\" instead of concentrated in a few eigenvalues. This fact led to relatively low cumulative explained variance ratio for lower values of M (reduction dimensions) and therefore a requirement to use higher M values to capture respectable variance from the original data set when performing dimension reduction. We finished the discussion by analyzing the correlation matrix of the target data set and highlighting relationships present between features, particularly relationships between swim, bike, run, and overall performance."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "779c798a75e127e3aa96660aebf9b74d3e571412428da99918dfc4cadf485d44"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}