{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Principal Component Analysis (PCA) Algorithm\n",
    "\n",
    "In this section, we walk through the steps behind implementing the PCA algorithm outlined in section 10.6 of the textbook. The algorithm will consist of four main steps:\n",
    "\n",
    "1. Centering the original data set samples around the origin\n",
    "2. Standardizing the data set based on standard deviation\n",
    "3. Computing the eigendecomposition of the covariance matrix from the centered, standardized data set\n",
    "4. Choosing the dimension for approximating the original space\n",
    "\n",
    "\n",
    "#### Assumptions\n",
    "\n",
    "* The data matrix X is structured such that rows are attributes and columns are samples\n",
    "* The number of rows in data matrix X is less than the number of columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src/')\n",
    "from PCA import PCA"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use the following data matrix below throughout the development of the algorithm to demonstrate intermediate steps."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# sample data matrix for testing/demonstration\n",
    "\n",
    "A = np.array([\n",
    "    [2,8,2,1,5],\n",
    "    [8,7,2,2,6],\n",
    "    [4,0,5,0,4]\n",
    "])\n",
    "\n",
    "print(A)\n",
    "print(A.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[2 8 2 1 5]\n",
      " [8 7 2 2 6]\n",
      " [4 0 5 0 4]]\n",
      "(3, 5)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Centering the Data Matrix\n",
    "\n",
    "Per the algorithm for PCA in section 10.6 of the book, we must first center the data matrix so that each dimension has a mean of 0. We achieve this by computing the mean of each dimension (row) and then subtracting all elements in each row by the respective row's mean value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "D, samples = A.shape\n",
    "rowmeans = np.mean(A, axis=1)\n",
    "offsetmatrix = np.repeat(rowmeans, samples, axis=0).reshape((D,samples))\n",
    "centered = A - offsetmatrix\n",
    "\n",
    "print(centered)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-1.6  4.4 -1.6 -2.6  1.4]\n",
      " [ 3.   2.  -3.  -3.   1. ]\n",
      " [ 1.4 -2.6  2.4 -2.6  1.4]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Standardization\n",
    "\n",
    "The next step in the PCA algorithm involves standardizing each component of the data matrix by dividing by the component's respective standard deviation. We show this behavior below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "rowstds = np.std(centered, axis=1)\n",
    "standardized = (centered.T / rowstds).T\n",
    "\n",
    "print(standardized)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-0.62092042  1.70753116 -0.62092042 -1.00899568  0.54330537]\n",
      " [ 1.18585412  0.79056942 -1.18585412 -1.18585412  0.39528471]\n",
      " [ 0.64993368 -1.2070197   1.11417203 -1.2070197   0.64993368]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Eigendecomposition of the Covariance Matrix\n",
    "\n",
    "We must first find the covariance matrix of the centered and standardized data array. We then compute the eigendecomposition of this covariance matrix. Following this, we can choose a desired number of dimensions to use for dimensionality reduction."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "covmatrix = np.cov(standardized)\n",
    "print(covmatrix)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 1.25        0.69030098 -0.39635072]\n",
      " [ 0.69030098  1.25        0.04587658]\n",
      " [-0.39635072  0.04587658  1.25      ]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# Compute the eigenvalues and unit-length eigenvectors of the standardized covariance matrix\n",
    "# The eigenvalues and vectors are ordered in ascending order\n",
    "\n",
    "res = np.linalg.eigh(covmatrix)\n",
    "\n",
    "# flip the order so the eigenvalues and vectors are sorted in descending order based on eigenvalue\n",
    "eigvals = np.flip(res[0])\n",
    "eigvecs = np.flip(res[1], axis=1)\n",
    "\n",
    "print(eigvals)\n",
    "print(eigvecs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2.026786  1.2895867 0.4336273]\n",
      "[[ 0.71551818 -0.02918716 -0.69798413]\n",
      " [ 0.61644272  0.4964592   0.61116826]\n",
      " [-0.32868237  0.86756923 -0.3732178 ]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We show below how we execute dimensionality reduction. We simply choose the same number of eigenvectors (descending order by weight) as the number dimensions we desire, and we represent these in a matrix as column vectors. We call this matrix B."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "DESIRED_DIMENSIONS = 2\n",
    "\n",
    "B = eigvecs[:, 0:DESIRED_DIMENSIONS]\n",
    "print(B)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.71551818 -0.02918716]\n",
      " [ 0.61644272  0.4964592 ]\n",
      " [-0.32868237  0.86756923]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Projection\n",
    "\n",
    "Using our dimension-reducing matrix, we can take vectors from the original space and project them onto the principal subspace with fewer dimensions than the original space. To express this projection in the original space, we multiply by the original standard deviation and add the mean of each for each vector component."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# x = a vector from the original space\n",
    "\n",
    "x = np.array([1,1,1])\n",
    "\n",
    "\n",
    "# x_standardized = x tranformed into centered, standardized space\n",
    "\n",
    "x_standardized = (x - rowmeans) / rowstds\n",
    "\n",
    "\n",
    "# x_principal = the vector obtained by transforming x_standardized into the principal subspace\n",
    "\n",
    "x_principal = B @ (B.T @ x_standardized)\n",
    "\n",
    "\n",
    "# x_approx = x_principal transformed back into the original space ~> An approximation of x after dimension reduction\n",
    "\n",
    "x_approx = (x_principal * rowstds) + rowmeans\n",
    "\n",
    "print(\"x =\", x)\n",
    "print(\"x_standardized =\", x_standardized)\n",
    "print(\"x_principal =\", x_principal)\n",
    "print(\"x_approx =\", x_approx)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x = [1 1 1]\n",
      "x_standardized = [-1.00899568 -1.58113883 -0.74278135]\n",
      "x_principal = [-0.99842797 -1.59039212 -0.73713071]\n",
      "x_approx = [1.02723108 0.97659083 1.01217185]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that in this example the approximation is quite close to the original sample after dimension reduction. \n",
    "\n",
    "## Implementation\n",
    "\n",
    "Now that we have illustrated the steps, we show the implementation of this procedure as a Python class.\n",
    "\n",
    "```python\n",
    "class PCA:\n",
    "    def __init__(self, X, dimensions):\n",
    "        # center the data matrix\n",
    "        D, samples = X.shape\n",
    "        self.rowmeans = np.mean(X, axis=1)\n",
    "        centered = X - np.repeat(self.rowmeans, samples, axis=0).reshape((D,samples))\n",
    "\n",
    "        # standardize the centered data matrix\n",
    "        self.rowstds = np.std(centered, axis=1)\n",
    "        standardized = (centered.T / self.rowstds).T\n",
    "\n",
    "        # compute the covariance matrix\n",
    "        self.covmatrix = np.cov(standardized)\n",
    "\n",
    "        # compute the eigendecomposition of the covariance matrix\n",
    "        res = np.linalg.eigh(self.covmatrix)\n",
    "        eigvals = np.flip(res[0])\n",
    "        eigvecs = np.flip(res[1], axis=1)\n",
    "\n",
    "        # compute B\n",
    "        self.B = eigvecs[:, 0:dimensions]\n",
    "\n",
    "    def reducer(self, x):\n",
    "        x_standardized = (x - self.rowmeans) / self.rowstds\n",
    "        x_principal = self.B @ (self.B.T @ x_standardized)\n",
    "        x_reduced = (x_principal * self.rowstds) + self.rowmeans\n",
    "        return x_reduced\n",
    "\n",
    "    def covariance_matrix(self):\n",
    "        return self.covmatrix\n",
    "```\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "\n",
    "\n",
    "pca = sklearn.decomposition.PCA(n_components=DESIRED_DIMENSIONS, svd_solver='full')\n",
    "pca.fit(standardized.T)\n",
    "print(pca.get_covariance())\n",
    "print(pca.inverse_transform(pca.transform(x.reshape(1,-1))))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 1.25        0.69030098 -0.39635072]\n",
      " [ 0.69030098  1.25        0.04587658]\n",
      " [-0.39635072  0.04587658  1.25      ]]\n",
      "[[0.6789038  1.28115797 0.82830725]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "pca_ = PCA(A, DESIRED_DIMENSIONS)\n",
    "print(pca_.covariance_matrix())\n",
    "print(pca_.reducer(x))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 1.25        0.69030098 -0.39635072]\n",
      " [ 0.69030098  1.25        0.04587658]\n",
      " [-0.39635072  0.04587658  1.25      ]]\n",
      "[1.02723108 0.97659083 1.01217185]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "interpreter": {
   "hash": "779c798a75e127e3aa96660aebf9b74d3e571412428da99918dfc4cadf485d44"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}